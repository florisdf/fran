{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c16141f-fd30-4fa1-8262-93c859157364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import sys; sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import transforms, functional as F\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from models.fran import FRAN\n",
    "from datasets.fran_dataset import FRANDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6064dfdf-0daa-4f34-8081-5a0aafbfb632",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dicts = torch.load('../ckpts/8ij6enbo_last.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aaf30c-808b-4d83-91d0-2a59dcf0779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fran = FRAN('zeros')\n",
    "\n",
    "fran.load_state_dict(state_dicts['FRAN'])\n",
    "fran.eval().cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e48626-d65a-4241-8070-1373a4dd4f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ccrop_size = (512, 512)\n",
    "tfm = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.CenterCrop(ccrop_size),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "inv_norm = transforms.Normalize([-1, -1, -1.], [2, 2, 2.])\n",
    "\n",
    "ds_val = FRANDataset(\n",
    "    data_root='../data/FRAN_dataset/',\n",
    "    is_val=True,\n",
    "    transform=tfm,\n",
    "    num_folds=5,\n",
    "    val_fold=0,\n",
    "    n_subsample=40,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d38984e-d40c-4315-bf35-6f567390aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = randint(0, len(ds_val) - 1)\n",
    "src_img, src_age, _, _ = ds_val[idx]\n",
    "src_img = src_img.cuda()[None, ...]\n",
    "\n",
    "tgt_ages = [20, 40, 60, 80]\n",
    "\n",
    "reaged_ims = []\n",
    "\n",
    "for tgt_age in tgt_ages:\n",
    "    src_age_map = torch.ones_like(src_img[:, :1, ...]) * src_age\n",
    "    tgt_age_map = torch.ones_like(src_age_map) * tgt_age\n",
    "\n",
    "    with torch.no_grad():\n",
    "        out = fran(src_img, src_age_map, tgt_age_map)[0].cpu()\n",
    "    \n",
    "    im = F.to_pil_image(inv_norm(out).clip(min=0, max=1)).resize((128, 128))\n",
    "    reaged_ims.append(im)\n",
    "\n",
    "Image.fromarray(np.concatenate(reaged_ims, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab48977-91b6-41ff-9318-433af2dfef1e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
