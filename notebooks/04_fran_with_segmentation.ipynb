{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c16141f-fd30-4fa1-8262-93c859157364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import transforms, functional as F\n",
    "from PIL import Image\n",
    "\n",
    "from models.bisenet import BiSeNet\n",
    "from models.fran import FRAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba40013-143f-4d4e-8a6f-f0dc2585e3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "im = Image.open('floris2.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d6686a-d580-4391-8258-679a115cd31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "bisenet = BiSeNet(n_classes=19)\n",
    "bisenet.load_state_dict(torch.load('../pretrained_models/bisenet_79999_iter.pth'))\n",
    "bisenet.eval().cuda();\n",
    "\n",
    "tfm_bise = transforms.Compose([\n",
    "    transforms.Resize(512),\n",
    "    transforms.CenterCrop(512),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "def get_face_mask(im):\n",
    "    bise_input = tfm_bise(im)[None, ...].cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        bise_output = bisenet(bise_input)[0][0].argmax(0).cpu().numpy()\n",
    "    mask = ~np.isin(bise_output, [0, 16, 17])\n",
    "    mask_im = Image.fromarray((mask * 255).astype(np.uint8)).resize((1024, 1024))\n",
    "    mask = np.array(mask_im) / 255\n",
    "\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aaf30c-808b-4d83-91d0-2a59dcf0779a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fran = FRAN(padding_mode='zeros')\n",
    "state_dicts = torch.load('/apollo/fdf/projects/fran/ckpts/8ij6enbo_ep11.pth')\n",
    "fran.load_state_dict(state_dicts['FRAN'])\n",
    "fran.eval().cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e48626-d65a-4241-8070-1373a4dd4f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1024\n",
    "\n",
    "tfm = transforms.Compose([\n",
    "    transforms.Resize(input_size),\n",
    "    transforms.CenterCrop(input_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "inv_norm = transforms.Normalize([-1, -1, -1.], [2, 2, 2.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d38984e-d40c-4315-bf35-6f567390aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = get_face_mask(im)\n",
    "t = tfm(im)[None, ...].cuda()\n",
    "\n",
    "src_age = 29\n",
    "tgt_ages = [80]\n",
    "\n",
    "reaged_ims = []\n",
    "\n",
    "for tgt_age in tgt_ages:\n",
    "    src_age_map = torch.ones((input_size, input_size)) * src_age\n",
    "    tgt_age_map = torch.tensor((mask * (tgt_age - src_age)) + src_age).float()\n",
    "    src_age_map = F.center_crop(src_age_map[None, None, ...], input_size).cuda()\n",
    "    tgt_age_map = F.center_crop(tgt_age_map[None, None, ...], input_size).cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        out = fran(t, src_age_map, tgt_age_map)[0].cpu()\n",
    "    \n",
    "    im_out = F.to_pil_image(inv_norm(out).clip(min=0, max=1)).resize((512, 512))\n",
    "    reaged_ims.append(im_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a2ea67-ff8b-4fc1-8ebc-012c865aef1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.fromarray(np.concatenate(reaged_ims, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab48977-91b6-41ff-9318-433af2dfef1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "\n",
    "def get_video_frames(video_path):\n",
    "    frames = []\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = frame[..., ::-1]\n",
    "        frames.append(Image.fromarray(frame))\n",
    "\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc96b239-838b-43ef-8f09-b00bcf2e9ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = get_video_frames('test.mov')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d538aa92-3aa8-47c7-851f-6134aafcf07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "src_age = 29\n",
    "tgt_age = 50\n",
    "\n",
    "\n",
    "src_age_map = torch.ones((input_size, input_size)) * src_age\n",
    "tgt_age_map = torch.ones_like(src_age_map) * tgt_age\n",
    "src_age_map = F.center_crop(src_age_map[None, None, ...], input_size).cuda()\n",
    "tgt_age_map = F.center_crop(tgt_age_map[None, None, ...], input_size).cuda()\n",
    "\n",
    "\n",
    "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "out = cv2.VideoWriter(f'output_fran_{tgt_age}.mp4', fourcc, 30.0, (input_size, input_size))\n",
    "\n",
    "resize_crop = transforms.Compose([\n",
    "    transforms.Resize(input_size),\n",
    "    transforms.CenterCrop(input_size),\n",
    "])\n",
    "\n",
    "for im in tqdm(frames):\n",
    "    t = tfm(im)[None, ...].cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        t_re_aged = fran(t, src_age_map, tgt_age_map)[0].cpu()\n",
    "\n",
    "    f_mask = get_face_mask(im)\n",
    "    im_re_aged = F.to_pil_image(inv_norm(t_re_aged).clip(min=0, max=1))\n",
    "\n",
    "    im_re_aged = np.array(im_re_aged) * f_mask[..., None]\n",
    "    im_bg = np.array(resize_crop(im)) * (1 - f_mask)[..., None]\n",
    "    im_out = (im_bg + im_re_aged).astype(np.uint8)\n",
    "    \n",
    "    out.write(im_out[..., ::-1])\n",
    "\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff43f96-fadd-4c6e-80ca-d3aff76b65de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
